{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen2-7B-Instruct 微调\n",
    "微调的基本方法参照 [Qwen2-7B-Instruct Lora 微调](https://blog.csdn.net/xiaobing259/article/details/140594017)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri Aug  9 11:51:38 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A100 80GB PCIe          Off | 00000000:4F:00.0 Off |                    0 |\n",
      "| N/A   68C    P0             287W / 300W |  48472MiB / 81920MiB |     88%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  NVIDIA A100 80GB PCIe          Off | 00000000:52:00.0 Off |                    0 |\n",
      "| N/A   72C    P0             294W / 300W |  46678MiB / 81920MiB |     93%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  NVIDIA A100 80GB PCIe          Off | 00000000:56:00.0 Off |                    0 |\n",
      "| N/A   70C    P0             272W / 300W |  53307MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  NVIDIA A100 80GB PCIe          Off | 00000000:57:00.0 Off |                    0 |\n",
      "| N/A   37C    P0              68W / 300W |  71489MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   4  NVIDIA A100 80GB PCIe          Off | 00000000:CE:00.0 Off |                    0 |\n",
      "| N/A   84C    P0             212W / 300W |  35415MiB / 81920MiB |     78%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   5  NVIDIA A100 80GB PCIe          Off | 00000000:D1:00.0 Off |                    0 |\n",
      "| N/A   81C    P0             298W / 300W |  60089MiB / 81920MiB |    100%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   6  NVIDIA A100 80GB PCIe          Off | 00000000:D5:00.0 Off |                    0 |\n",
      "| N/A   71C    P0             302W / 300W |  25405MiB / 81920MiB |     89%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   7  NVIDIA A100 80GB PCIe          Off | 00000000:D6:00.0 Off |                    0 |\n",
      "| N/A   39C    P0              66W / 300W |  70099MiB / 81920MiB |      0%      Default |\n",
      "|                                         |                      |             Disabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   1095299      C   python                                    46388MiB |\n",
      "|    0   N/A  N/A   2058100      C   python                                     2072MiB |\n",
      "|    1   N/A  N/A   1109998      C   python                                    46252MiB |\n",
      "|    1   N/A  N/A   3030986      C   python                                      414MiB |\n",
      "|    2   N/A  N/A   1318570      C   ...iniconda3/envs/datawhale/bin/python    53298MiB |\n",
      "|    3   N/A  N/A   2758239      C   python                                    71472MiB |\n",
      "|    4   N/A  N/A   1297818      C   python                                    21708MiB |\n",
      "|    4   N/A  N/A   1805895      C   python                                     1080MiB |\n",
      "|    4   N/A  N/A   1806474      C   .../miniconda3/envs/Wav2Lip/bin/python     1846MiB |\n",
      "|    4   N/A  N/A   1806475      C   .../miniconda3/envs/Wav2Lip/bin/python     7478MiB |\n",
      "|    4   N/A  N/A   1806476      C   .../miniconda3/envs/Wav2Lip/bin/python     1866MiB |\n",
      "|    4   N/A  N/A   1806477      C   .../miniconda3/envs/Wav2Lip/bin/python     1410MiB |\n",
      "|    5   N/A  N/A   1226879      C   ...iniconda3/envs/datawhale/bin/python    60080MiB |\n",
      "|    6   N/A  N/A   2834615      C   ...iniconda3/envs/datawhale/bin/python    25396MiB |\n",
      "|    7   N/A  N/A   3009755      C   python3                                   70090MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "cuda:4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import json\n",
    "import torch\n",
    "import optuna\n",
    "from datasets import Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, DataCollatorForSeq2Seq, TrainingArguments, Trainer, GenerationConfig, EarlyStoppingCallback\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from sklearn.model_selection import train_test_split\n",
    "# from data_processing.utils import extract_from_output\n",
    "\n",
    "!nvidia-smi\n",
    "\n",
    "GPU_ID = 4\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = f\"{GPU_ID}\"  # 只使用一张显卡\n",
    "device = torch.device(f\"cuda:{GPU_ID}\" if torch.cuda.is_available() else \"cpu\")  # 指定使用的设备\n",
    "device_map = {\"\": f\"cuda:{GPU_ID}\"}\n",
    "print(device)\n",
    "\n",
    "# 多 GPU\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1'\n",
    "# device = torch.device(f\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# device_map = {: ''}\n",
    "\n",
    "# 任务代号\n",
    "TASK_ID = '5000'\n",
    "# torch.cuda.device_count()\n",
    "# # 指定使用第1号GPU\n",
    "# torch.cuda.set_device(0)\n",
    "# # 检查设置是否生效\n",
    "# print(torch.cuda.current_device())  # 应该输出1\n",
    "# print(torch.cuda.get_device_name(0))  # 输出第1号GPU的名称"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The DataFrame has 35149 rows.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['instruction', 'input', 'output', '__index_level_0__'],\n",
       "    num_rows: 4500\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # 读取原数据集\n",
    "# df1 = pd.read_json('/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/raw/round1_train_data_instruction.json')\n",
    "# /data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/train_data_raw_new_inst.json  # 包含原数据集噪声\n",
    "# 读取新增数据集\n",
    "# df2 = pd.read_json('/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/new/train_data_new_inst.json')  # 人工减弱噪声后的数据集\n",
    "\n",
    "# # # 读取生成数据集\n",
    "# df3 = pd.read_json('/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/gpt/gpt4_4590_806_inst.json')\n",
    "\n",
    "# # 合并两个 DataFrame\n",
    "# merged_df = pd.concat([df2, df3], ignore_index=True)\n",
    "# print(len(merged_df))\n",
    "\n",
    "# 数据集（指令集）路径\n",
    "data_path = '/data/disk4/home/chenrui/ai-project/logical_reasoning/LLaMA-Factory/data/logical_problems_large.json'\n",
    "\n",
    "# # 合并好的指令集\n",
    "merged_df = pd.read_json(data_path)\n",
    "\n",
    "# 检查DataFrame的长度\n",
    "num_rows = len(merged_df)\n",
    "print(f\"The DataFrame has {num_rows} rows.\")\n",
    "\n",
    "# 如果DataFrame的行数大于或等于5000，随机抽取5000行\n",
    "if num_rows >= 5000:\n",
    "    random_seed = 42\n",
    "    sample_df = merged_df.sample(n=5000, random_state=random_seed)\n",
    "else:\n",
    "    print(\"The DataFrame has fewer than 5000 rows. Returning the entire DataFrame.\")\n",
    "    sample_df = merged_df\n",
    "\n",
    "# 划分训练集和验证集\n",
    "train_data, val_data = train_test_split(sample_df, test_size=0.1, random_state=42)  # 80% 训练，20% 验证\n",
    "# print(val_data)\n",
    "\n",
    "# 将验证集保存为JSON格式到tmp文件夹\n",
    "tmp_dir = '/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/tmp'\n",
    "os.makedirs(tmp_dir, exist_ok=True)  # 创建tmp文件夹（如果不存在的话）\n",
    "val_data.to_json(os.path.join(tmp_dir, 'validation_data.json'), orient='records', force_ascii=False)\n",
    "\n",
    "# 将数据集转换为Dataset对象\n",
    "train_ds = Dataset.from_pandas(train_data)\n",
    "val_ds = Dataset.from_pandas(val_data)\n",
    "\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 加载分词器和模型\n",
    "model_path = '/data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map=device_map, torch_dtype=torch.bfloat16)\n",
    "model.enable_input_require_grads() # 开启梯度检查点时，要执行该方法\n",
    "model.dtype # 查看精度\n",
    "print(next(model.parameters()).device)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 4500/4500 [00:11<00:00, 407.47 examples/s]\n",
      "Map: 100%|██████████| 500/500 [00:01<00:00, 332.69 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(Dataset({\n",
       "     features: ['input_ids', 'attention_mask', 'labels'],\n",
       "     num_rows: 4500\n",
       " }),\n",
       " '<|im_start|>system\\n你是一个逻辑推理专家，擅长解决逻辑推理问题。<|im_end|>\\n<|im_start|>user\\n\\n你是一个逻辑推理专家，擅长解决逻辑推理问题。以下是一个逻辑推理的题目，形式为单项选择题。所有的问题都是（close-world assumption）闭世界假设，即未观测事实都为假。请逐步分析问题，最终只输出答案对应的选项字母，如\"A\"。题目如下：\\n\\n### 题目:\\n**逻辑推理测试题：**\\n\\n以下是关于几种不同的职业人员的假设及其特性的推理系统。请根据已知条件和推理规则回答问题。\\n\\n已知条件：\\n1. 穿白大褂的是医生。\\n2. 持有教鞭的是教师。\\n3. 穿制服并且戴帽子的是警察。\\n4. 如果一个人在图书馆工作，那么他是图书管理员。\\n5. 假设“John”穿着白大褂。\\n6. 假设“Emma”持有教鞭。\\n7. 假设“Oliver”穿着制服并且戴着帽子。\\n8. 假设“Alice”在图书馆工作。\\n\\n请回答以下选择题：\\n\\n### 问题:\\n选择题 2：\\nEmma的职业是什么？\\nA. 医生\\nB. 教师\\nC. 警察\\nD. 图书管理员\\n<|im_end|>\\n<|im_start|>assistant\\nB<|endoftext|>',\n",
       " 'D<|endoftext|>')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def process_func(example):\n",
    "    MAX_LENGTH = 1800    # Llama分词器会将一个中文字切分为多个token，因此需要放开一些最大长度，保证数据的完整性\n",
    "    input_ids, attention_mask, labels = [], [], []\n",
    "    instruction = tokenizer(f\"<|im_start|>system\\n你是一个逻辑推理专家，擅长解决逻辑推理问题。<|im_end|>\\n<|im_start|>user\\n{example['instruction'] + example['input']}<|im_end|>\\n<|im_start|>assistant\\n\", add_special_tokens=False)  # add_special_tokens 不在开头加 special_tokens\n",
    "    response = tokenizer(f\"{example['output']}\", add_special_tokens=False)\n",
    "    input_ids = instruction[\"input_ids\"] + response[\"input_ids\"] + [tokenizer.pad_token_id]\n",
    "    attention_mask = instruction[\"attention_mask\"] + response[\"attention_mask\"] + [1]  # 因为eos token咱们也是要关注的所以 补充为1\n",
    "    labels = [-100] * len(instruction[\"input_ids\"]) + response[\"input_ids\"] + [tokenizer.pad_token_id]  \n",
    "    if len(input_ids) > MAX_LENGTH:  # 做一个截断\n",
    "        input_ids = input_ids[:MAX_LENGTH]\n",
    "        attention_mask = attention_mask[:MAX_LENGTH]\n",
    "        labels = labels[:MAX_LENGTH]\n",
    "    return {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"attention_mask\": attention_mask,\n",
    "        \"labels\": labels\n",
    "    }\n",
    "# 处理训练集和验证集\n",
    "train_ds = train_ds.map(process_func, remove_columns=train_ds.column_names)\n",
    "val_ds = val_ds.map(process_func, remove_columns=val_ds.column_names)\n",
    "num_train_samples = len(train_ds)\n",
    "train_ds, tokenizer.decode(train_ds[0]['input_ids']), tokenizer.decode(list(filter(lambda x: x != -100, train_ds[1][\"labels\"])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,185,088 || all params: 7,635,801,600 || trainable%: 0.2643\n"
     ]
    }
   ],
   "source": [
    "# 设置 lora 参数\n",
    "config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM, \n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    inference_mode=False, # 训练模式\n",
    "    r=8, # Lora 秩\n",
    "    lora_alpha=16,  # Lora alaph，具体作用参见 Lora 原理\n",
    "    lora_dropout=0.3  # Dropout 比例\n",
    ")\n",
    "model = get_peft_model(model, config)\n",
    "\n",
    "# 查看可训练参数\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# 多 GPU 分布式训练\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = torch.nn.DataParallel(model)\n",
    "#     print(torch.cuda.device_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training steps: 1124\n"
     ]
    }
   ],
   "source": [
    "# 自定义 Trainer 类\n",
    "class CustomTrainer(Trainer):\n",
    "    def training_step(self, model, inputs):\n",
    "        loss = super().training_step(model, inputs)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # 梯度裁剪\n",
    "        return loss\n",
    "\n",
    "# 设置训练轮次和批量大小\n",
    "per_device_train_batch_size = 4\n",
    "per_device_eval_batch_size = 4\n",
    "num_train_epochs = 4\n",
    "gradient_accumulation_steps = 4  # 2 次前向传播才进行一次反向传播和参数更新，有利于防止梯度爆炸\n",
    "\n",
    "# 查看训练总步数\n",
    "total_training_steps = (num_train_samples // per_device_train_batch_size // gradient_accumulation_steps) * num_train_epochs\n",
    "print(f\"Total training steps: {total_training_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 设置训练超参数\n",
    "args = TrainingArguments(\n",
    "    output_dir=f\"./checkpoints/Qwen2_math_7B_instruct_lora_{TASK_ID}\",\n",
    "    per_device_train_batch_size=per_device_train_batch_size,\n",
    "    per_device_eval_batch_size=per_device_eval_batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    logging_steps=10,\n",
    "    save_steps=200,\n",
    "    learning_rate=1e-4,\n",
    "    save_on_each_node=True,\n",
    "    gradient_checkpointing=True,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    warmup_steps=100,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    evaluation_strategy=\"steps\",  # 设置评估策略\n",
    "    eval_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    # fp16=True,  # 使用混合精度训练，防止梯度爆炸\n",
    "    load_best_model_at_end=True,  # 在训练结束时加载最佳模型\n",
    "    metric_for_best_model=\"eval_loss\",  # 监控的指标\n",
    "    greater_is_better=False,  # 因为我们希望损失最小化\n",
    "    logging_dir='./logs',  # TensorBoard 日志目录\n",
    "    report_to=\"tensorboard\",  # 启用 TensorBoard\n",
    "    # dataloader_num_workers=4,\n",
    "    # distributed_data_parallel=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2024-08-09 11:52:02,955] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "\u001b[93m [WARNING] \u001b[0m async_io requires the dev libaio .so object and headers but these were not found.\n",
      "\u001b[93m [WARNING] \u001b[0m async_io: please install the libaio-dev package with apt\n",
      "\u001b[93m [WARNING] \u001b[0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.\n",
      "\u001b[93m [WARNING] \u001b[0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH\n",
      "\u001b[93m [WARNING] \u001b[0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.4\n",
      "\u001b[93m [WARNING] \u001b[0m using untested triton version (3.0.0), only 1.0.0 is known to be compatible\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:49: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "  def forward(ctx, input, weight, bias=None):\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/deepspeed/runtime/zero/linear.py:67: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "  def backward(ctx, grad_output):\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='900' max='1124' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 900/1124 1:00:29 < 15:05, 0.25 it/s, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.387700</td>\n",
       "      <td>0.387831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.323100</td>\n",
       "      <td>0.304432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.176500</td>\n",
       "      <td>0.328981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.207000</td>\n",
       "      <td>0.295358</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.133800</td>\n",
       "      <td>0.302061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.067900</td>\n",
       "      <td>0.363511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.031600</td>\n",
       "      <td>0.432238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.037600</td>\n",
       "      <td>0.461664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.025700</td>\n",
       "      <td>0.468061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/_dynamo/eval_frame.py:600: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n",
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/torch/utils/checkpoint.py:295: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with torch.enable_grad(), device_autocast_ctx, torch.cpu.amp.autocast(**ctx.cpu_autocast_kwargs):  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=900, training_loss=0.5953734336958991, metrics={'train_runtime': 3635.0512, 'train_samples_per_second': 4.952, 'train_steps_per_second': 0.309, 'total_flos': 1.8455826108716237e+17, 'train_loss': 0.5953734336958991, 'epoch': 3.2})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 训练 可用 CustomTrainer\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=train_ds,\n",
    "    eval_dataset=val_ds,  # 添加验证集\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer=tokenizer, padding=True),\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)],  # 设置早停法，耐心为 3\n",
    ")\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/disk4/home/chenrui/miniconda3/envs/datawhale/lib/python3.10/site-packages/peft/utils/save_and_load.py:195: UserWarning: Could not find a config file in /data/disk4/home/chenrui/.cache/modelscope/hub/qwen/Qwen2-Math-7B-Instruct - will assume that the vocabulary was not modified.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "\n",
      "NOTE: Using experimental fast data loading logic. To disable, pass\n",
      "    \"--load_fast=false\" and report issues on GitHub. More details:\n",
      "    https://github.com/tensorflow/tensorboard/issues/4784\n",
      "\n",
      "TensorBoard 2.17.0 at http://172.20.2.1:6006/ (Press CTRL+C to quit)\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model(f\"./output/Qwen2_math_7B_instruct_lora_{TASK_ID}_final\")\n",
    "!tensorboard --logdir=./logs --host=172.20.2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用微调后的模型对验证集进行推理\n",
    "此时可以重启 Jupyter Kernel，释放显存"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # 只使用第一张显卡\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")  # 指定使用的设备\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 lora 权重\n",
    "model_path = '/data/disk4/home/chenrui/ai-project/Qwen2-7B-Instruct'\n",
    "lora_path = '/data/disk4/home/chenrui/ai-project/logical_reasoning/output/Qwen2_7B_instruct_lora_1_final'  # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map={\"\":\"cuda:2\"}, torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path).to(device)  # 将 lora 权重加进原模型\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# 读取JSONL文件\n",
    "data = []\n",
    "input_file = '/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/tmp/validation_data.json'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)  # 读取 JSON 数据\n",
    "print(f\"The size of validation data: {len(data)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量推理并计算正确率\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    instruction = item['instruction']\n",
    "    input_text = item['input']\n",
    "    expected_output = item['output']\n",
    "    # match = re.findall(r'[A-G]', output)\n",
    "    # if match:\n",
    "    #     expected_output = match[-1]\n",
    "\n",
    "    prompt = instruction + input_text\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\", return_dict=True).to(device)\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "    \n",
    "    # 存储多次调用的输出\n",
    "    outputs_list = []\n",
    "    \n",
    "    # 三次调用模型\n",
    "    for i in range(3):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **gen_kwargs)\n",
    "            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "            output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            match = re.findall(r'[A-G]', output)\n",
    "            if match:\n",
    "                output = match[-1]\n",
    "            outputs_list.append(output)\n",
    "            print(f\"Model output {i}: {output}\")\n",
    "\n",
    "    # 进行多路投票\n",
    "    vote_counts = Counter(outputs_list)\n",
    "    final_output = vote_counts.most_common(1)[0][0]  # 选择出现次数最多的结果\n",
    "    print(f\"Final voted output: {final_output}, Expected output: {expected_output}\")\n",
    "\n",
    "    # 比较预测答案和正确答案\n",
    "    if final_output == expected_output:\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    # 每处理 n 条数据输出一次实时的正确率\n",
    "    if (total_count) % 50 == 0:\n",
    "        accuracy = correct_count / total_count\n",
    "        print(f\"Processed {total_count} items, Current Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# 最终正确率\n",
    "accuracy = correct_count / total_count\n",
    "print(f\"Final Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除生成的临时文件\n",
    "# if os.path.exists(tmp_dir):\n",
    "#     shutil.rmtree(tmp_dir)\n",
    "#     os.makedirs(tmp_dir)\n",
    "#     print(f'The folder {tmp_dir} has been cleared.')\n",
    "# else:\n",
    "#     print(f'The folder {tmp_dir} does not exist.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 模型合并存储\n",
    "# new_model_directory = \"./merged_model_an\"\n",
    "# merged_model = model.merge_and_unload()\n",
    "\n",
    "# # 将权重保存为safetensors格式的权重, 且每个权重文件最大不超过2GB(2048MB)\n",
    "# merged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datawhale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
