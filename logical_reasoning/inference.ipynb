{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 用微调后的模型对验证集进行推理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pdb\n",
    "import json\n",
    "import torch\n",
    "import shutil\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from collections import Counter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"2\"  # 只使用第一张显卡\n",
    "device = torch.device(\"cuda:2\" if torch.cuda.is_available() else \"cpu\")  # 指定使用的设备\n",
    "\n",
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 lora 权重\n",
    "model_path = '/data/disk4/home/chenrui/ai-project/Qwen2-7B-Instruct'\n",
    "lora_path = '/data/disk4/home/chenrui/ai-project/logical_reasoning/checkpoints/Qwen2_7B_instruct_lora_with_thinking/checkpoint-300'  # 这里改称你的 lora 输出对应 checkpoint 地址\n",
    "\n",
    "# 加载模型\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map={\"\":\"cuda:2\"}, torch_dtype=torch.bfloat16, trust_remote_code=True).eval()\n",
    "model = PeftModel.from_pretrained(model, model_id=lora_path).to(device)  # 将 lora 权重加进原模型\n",
    "print(f\"Model is on device: {next(model.parameters()).device}\")\n",
    "\n",
    "# 加载tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# 读取JSONL文件\n",
    "data = []\n",
    "input_file = '/data/disk4/home/chenrui/ai-project/logical_reasoning/data/input/tmp/validation_data.json'\n",
    "with open(input_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)  # 读取 JSON 数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 批量推理并计算正确率\n",
    "correct_count = 0\n",
    "total_count = 0\n",
    "\n",
    "for idx, item in enumerate(data):\n",
    "    instruction = item['instruction']\n",
    "    input_text = item['input']\n",
    "    expected_output = item['output']\n",
    "    # match = re.findall(r'[A-G]', output)\n",
    "    # if match:\n",
    "    #     expected_output = match[-1]\n",
    "\n",
    "    prompt = instruction + input_text\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_tensors=\"pt\", return_dict=True).to(device)\n",
    "\n",
    "    gen_kwargs = {\"max_length\": 2500, \"do_sample\": True, \"top_k\": 1}\n",
    "    \n",
    "    # 存储多次调用的输出\n",
    "    outputs_list = []\n",
    "    \n",
    "    # 三次调用模型\n",
    "    for i in range(3):\n",
    "        with torch.no_grad():\n",
    "            outputs = model.generate(**inputs, **gen_kwargs)\n",
    "            outputs = outputs[:, inputs['input_ids'].shape[1]:]\n",
    "            output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            match = re.findall(r'[A-G]', output)\n",
    "            if match:\n",
    "                output = match[-1]\n",
    "            outputs_list.append(output)\n",
    "            print(f\"Model output {i}: {output}\")\n",
    "\n",
    "    # 进行多路投票\n",
    "    vote_counts = Counter(outputs_list)\n",
    "    final_output = vote_counts.most_common(1)[0][0]  # 选择出现次数最多的结果\n",
    "    print(f\"Final voted output: {final_output}, Expected output: {expected_output}\")\n",
    "\n",
    "    # 比较预测答案和正确答案\n",
    "    if final_output == expected_output:\n",
    "        correct_count += 1\n",
    "    total_count += 1\n",
    "\n",
    "    # 每处理 n 条数据输出一次实时的正确率\n",
    "    if (total_count) % 50 == 0:\n",
    "        accuracy = correct_count / total_count\n",
    "        print(f\"Processed {total_count} items, Current Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "# 最终正确率\n",
    "accuracy = correct_count / total_count\n",
    "print(f\"Final Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 清除生成的临时文件\n",
    "# if os.path.exists(tmp_dir):\n",
    "#     shutil.rmtree(tmp_dir)\n",
    "#     os.makedirs(tmp_dir)\n",
    "#     print(f'The folder {tmp_dir} has been cleared.')\n",
    "# else:\n",
    "#     print(f'The folder {tmp_dir} does not exist.')\n",
    "\n",
    "\n",
    "# # 模型合并存储\n",
    "# new_model_directory = \"./merged_model_an\"\n",
    "# merged_model = model.merge_and_unload()\n",
    "\n",
    "# # 将权重保存为safetensors格式的权重, 且每个权重文件最大不超过2GB(2048MB)\n",
    "# merged_model.save_pretrained(new_model_directory, max_shard_size=\"2048MB\", safe_serialization=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "datawhale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
