{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3：源大模型RAG实战"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 环境准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看已安装依赖\n",
    "! pip list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 安装 streamlit\n",
    "! pip install streamlit==1.24.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 模型下载"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:45:37.370790Z",
     "iopub.status.busy": "2024-07-13T10:45:37.370579Z",
     "iopub.status.idle": "2024-07-13T10:45:48.672100Z",
     "shell.execute_reply": "2024-07-13T10:45:48.671603Z",
     "shell.execute_reply.started": "2024-07-13T10:45:37.370773Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading: 100%|██████████| 190/190 [00:00<00:00, 328B/s]\n",
      "Downloading: 100%|██████████| 776/776 [00:00<00:00, 1.48kB/s]\n",
      "Downloading: 100%|██████████| 124/124 [00:00<00:00, 266B/s]\n",
      "Downloading: 100%|██████████| 47.0/47.0 [00:00<00:00, 92.2B/s]\n",
      "Downloading: 100%|██████████| 91.4M/91.4M [00:00<00:00, 120MB/s] \n",
      "Downloading: 100%|██████████| 349/349 [00:00<00:00, 715B/s]\n",
      "Downloading: 100%|██████████| 91.4M/91.4M [00:00<00:00, 123MB/s] \n",
      "Downloading: 100%|██████████| 27.5k/27.5k [00:00<00:00, 54.9kB/s]\n",
      "Downloading: 100%|██████████| 52.0/52.0 [00:00<00:00, 65.0B/s]\n",
      "Downloading: 100%|██████████| 125/125 [00:00<00:00, 253B/s]\n",
      "Downloading: 100%|██████████| 429k/429k [00:00<00:00, 703kB/s]\n",
      "Downloading: 100%|██████████| 367/367 [00:00<00:00, 759B/s]\n",
      "Downloading: 100%|██████████| 107k/107k [00:00<00:00, 220kB/s]\n"
     ]
    }
   ],
   "source": [
    "# 向量模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download(\"AI-ModelScope/bge-small-zh-v1.5\", cache_dir='.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:50:34.404766Z",
     "iopub.status.busy": "2024-07-13T10:50:34.404429Z",
     "iopub.status.idle": "2024-07-13T10:51:04.199514Z",
     "shell.execute_reply": "2024-07-13T10:51:04.199032Z",
     "shell.execute_reply.started": "2024-07-13T10:50:34.404746Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: 100%|██████████| 0.98k/0.98k [00:00<00:00, 1.91kB/s]\n",
      "Downloading: 100%|██████████| 0.98k/0.98k [00:00<00:00, 2.43kB/s]\n",
      "Downloading: 100%|██████████| 51.0/51.0 [00:00<00:00, 110B/s]\n",
      "Downloading: 100%|██████████| 1.29k/1.29k [00:00<00:00, 2.06kB/s]\n",
      "Downloading: 100%|██████████| 144/144 [00:00<00:00, 273B/s]\n",
      "Downloading: 0.00B [00:00, ?B/s]\n",
      "Downloading: 100%|██████████| 4.41G/4.41G [00:12<00:00, 391MB/s] \n",
      "Downloading: 100%|██████████| 7.61k/7.61k [00:00<00:00, 20.2kB/s]\n",
      "Downloading: 100%|██████████| 411/411 [00:00<00:00, 732B/s]\n",
      "Downloading: 100%|██████████| 2.06M/2.06M [00:00<00:00, 3.67MB/s]\n",
      "Downloading: 100%|██████████| 1.12k/1.12k [00:00<00:00, 2.22kB/s]\n",
      "Downloading: 100%|██████████| 52.0k/52.0k [00:00<00:00, 99.9kB/s]\n",
      "Downloading: 100%|██████████| 52.0k/52.0k [00:00<00:00, 131kB/s]\n"
     ]
    }
   ],
   "source": [
    "# 源大模型下载\n",
    "from modelscope import snapshot_download\n",
    "model_dir = snapshot_download('IEITYuan/Yuan2-2B-Mars-hf', cache_dir='.')\n",
    "# model_dir = snapshot_download('IEITYuan/Yuan2-2B-July-hf', cache_dir='.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 RAG实战"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:53:05.360239Z",
     "iopub.status.busy": "2024-07-13T10:53:05.359877Z",
     "iopub.status.idle": "2024-07-13T10:53:05.366148Z",
     "shell.execute_reply": "2024-07-13T10:53:05.365548Z",
     "shell.execute_reply.started": "2024-07-13T10:53:05.360217Z"
    }
   },
   "outputs": [],
   "source": [
    "# 导入所需的库\n",
    "from typing import List\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:53:07.662471Z",
     "iopub.status.busy": "2024-07-13T10:53:07.662153Z",
     "iopub.status.idle": "2024-07-13T10:53:07.668122Z",
     "shell.execute_reply": "2024-07-13T10:53:07.667323Z",
     "shell.execute_reply.started": "2024-07-13T10:53:07.662450Z"
    }
   },
   "outputs": [],
   "source": [
    "# 定义向量模型类\n",
    "class EmbeddingModel:\n",
    "    \"\"\"\n",
    "    class for EmbeddingModel\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, path: str) -> None:\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(path)\n",
    "\n",
    "        self.model = AutoModel.from_pretrained(path).cuda()\n",
    "        print(f'Loading EmbeddingModel from {path}.')\n",
    "\n",
    "    def get_embeddings(self, texts: List) -> List[float]:\n",
    "        \"\"\"\n",
    "        calculate embedding for text list\n",
    "        \"\"\"\n",
    "        encoded_input = self.tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
    "        encoded_input = {k: v.cuda() for k, v in encoded_input.items()}\n",
    "        with torch.no_grad():\n",
    "            model_output = self.model(**encoded_input)\n",
    "            sentence_embeddings = model_output[0][:, 0]\n",
    "        sentence_embeddings = torch.nn.functional.normalize(sentence_embeddings, p=2, dim=1)\n",
    "        return sentence_embeddings.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-13T10:53:30.590153Z",
     "iopub.status.busy": "2024-07-13T10:53:30.589807Z",
     "iopub.status.idle": "2024-07-13T10:53:31.000988Z",
     "shell.execute_reply": "2024-07-13T10:53:31.000429Z",
     "shell.execute_reply.started": "2024-07-13T10:53:30.590118Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create embedding model...\n",
      "Loading EmbeddingModel from ./AI-ModelScope/bge-small-zh-v1___5.\n"
     ]
    }
   ],
   "source": [
    "print(\"> Create embedding model...\")\n",
    "embed_model_path = './AI-ModelScope/bge-small-zh-v1___5'\n",
    "embed_model = EmbeddingModel(embed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:53:34.210450Z",
     "iopub.status.busy": "2024-07-13T10:53:34.210088Z",
     "iopub.status.idle": "2024-07-13T10:53:34.216300Z",
     "shell.execute_reply": "2024-07-13T10:53:34.215670Z",
     "shell.execute_reply.started": "2024-07-13T10:53:34.210429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义向量库索引类\n",
    "class VectorStoreIndex:\n",
    "    \"\"\"\n",
    "    class for VectorStoreIndex\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, doecment_path: str, embed_model: EmbeddingModel) -> None:\n",
    "        self.documents = []\n",
    "        for line in open(doecment_path, 'r', encoding='utf-8'):\n",
    "            line = line.strip()\n",
    "            self.documents.append(line)\n",
    "\n",
    "        self.embed_model = embed_model\n",
    "        self.vectors = self.embed_model.get_embeddings(self.documents)\n",
    "\n",
    "        print(f'Loading {len(self.documents)} documents for {doecment_path}.')\n",
    "\n",
    "    def get_similarity(self, vector1: List[float], vector2: List[float]) -> float:\n",
    "        \"\"\"\n",
    "        calculate cosine similarity between two vectors\n",
    "        \"\"\"\n",
    "        dot_product = np.dot(vector1, vector2)\n",
    "        magnitude = np.linalg.norm(vector1) * np.linalg.norm(vector2)\n",
    "        if not magnitude:\n",
    "            return 0\n",
    "        return dot_product / magnitude\n",
    "\n",
    "    def query(self, question: str, k: int = 1) -> List[str]:\n",
    "        question_vector = self.embed_model.get_embeddings([question])[0]\n",
    "        result = np.array([self.get_similarity(question_vector, vector) for vector in self.vectors])\n",
    "        return np.array(self.documents)[result.argsort()[-k:][::-1]].tolist() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:53:36.758435Z",
     "iopub.status.busy": "2024-07-13T10:53:36.758082Z",
     "iopub.status.idle": "2024-07-13T10:53:37.148188Z",
     "shell.execute_reply": "2024-07-13T10:53:37.147628Z",
     "shell.execute_reply.started": "2024-07-13T10:53:36.758415Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create index...\n",
      "Loading 3 documents for ./knowledge.txt.\n"
     ]
    }
   ],
   "source": [
    "print(\"> Create index...\")\n",
    "doecment_path = './knowledge.txt'\n",
    "index = VectorStoreIndex(doecment_path, embed_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:53:39.691993Z",
     "iopub.status.busy": "2024-07-13T10:53:39.691618Z",
     "iopub.status.idle": "2024-07-13T10:53:39.709507Z",
     "shell.execute_reply": "2024-07-13T10:53:39.708733Z",
     "shell.execute_reply.started": "2024-07-13T10:53:39.691967Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Question: 介绍一下广州大学\n",
      "> Context: ['广州大学（Guangzhou University），简称广大（GU），是由广东省广州市人民政府举办的全日制普通高等学校，实行省市共建、以市为主的办学体制，是国家“111计划”建设高校、广东省和广州市高水平大学重点建设高校。广州大学的办学历史可以追溯到1927年创办的私立广州大学；1951年并入华南联合大学；1983年筹备复办，1984年定名为广州大学；2000年7月，经教育部批准，与广州教育学院（1953年创办）、广州师范学院（1958年创办）、华南建设学院西院（1984年创办）、广州高等师范专科学校（1985年创办）合并组建成立新的广州大学。']\n"
     ]
    }
   ],
   "source": [
    "question = '介绍一下广州大学'\n",
    "print('> Question:', question)\n",
    "\n",
    "context = index.query(question)\n",
    "print('> Context:', context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:54:58.258735Z",
     "iopub.status.busy": "2024-07-13T10:54:58.258414Z",
     "iopub.status.idle": "2024-07-13T10:54:58.264257Z",
     "shell.execute_reply": "2024-07-13T10:54:58.263550Z",
     "shell.execute_reply.started": "2024-07-13T10:54:58.258716Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 定义大语言模型类\n",
    "class LLM:\n",
    "    \"\"\"\n",
    "    class for Yuan2.0 LLM\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_path: str) -> None:\n",
    "        print(\"Creat tokenizer...\")\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_path, add_eos_token=False, add_bos_token=False, eos_token='<eod>')\n",
    "        self.tokenizer.add_tokens(['<sep>', '<pad>', '<mask>', '<predict>', '<FIM_SUFFIX>', '<FIM_PREFIX>', '<FIM_MIDDLE>','<commit_before>','<commit_msg>','<commit_after>','<jupyter_start>','<jupyter_text>','<jupyter_code>','<jupyter_output>','<empty_output>'], special_tokens=True)\n",
    "\n",
    "        print(\"Creat model...\")\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True).cuda()\n",
    "\n",
    "        print(f'Loading Yuan2.0 model from {model_path}.')\n",
    "\n",
    "    def generate(self, question: str, context: List):\n",
    "        if context:\n",
    "            prompt = f'背景：{context}\\n问题：{question}\\n请基于背景，回答问题。'\n",
    "        else:\n",
    "            prompt = question\n",
    "\n",
    "        prompt += \"<sep>\"\n",
    "        inputs = self.tokenizer(prompt, return_tensors=\"pt\")[\"input_ids\"].cuda()\n",
    "        outputs = self.model.generate(inputs, do_sample=False, max_length=1024)\n",
    "        output = self.tokenizer.decode(outputs[0])\n",
    "\n",
    "        print(output.split(\"<sep>\")[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-07-13T10:55:35.033979Z",
     "iopub.status.busy": "2024-07-13T10:55:35.033665Z",
     "iopub.status.idle": "2024-07-13T10:55:37.823999Z",
     "shell.execute_reply": "2024-07-13T10:55:37.823487Z",
     "shell.execute_reply.started": "2024-07-13T10:55:35.033960Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Create Yuan2.0 LLM...\n",
      "Creat tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama_fast.LlamaTokenizerFast'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creat model...\n",
      "Loading Yuan2.0 model from ./IEITYuan/Yuan2-2B-Mars-hf.\n"
     ]
    }
   ],
   "source": [
    "print(\"> Create Yuan2.0 LLM...\")\n",
    "model_path = './IEITYuan/Yuan2-2B-Mars-hf'\n",
    "# model_path = './IEITYuan/Yuan2-2B-July-hf'\n",
    "llm = LLM(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2024-07-13T10:55:49.752284Z",
     "iopub.status.busy": "2024-07-13T10:55:49.751914Z",
     "iopub.status.idle": "2024-07-13T10:56:03.592847Z",
     "shell.execute_reply": "2024-07-13T10:56:03.592357Z",
     "shell.execute_reply.started": "2024-07-13T10:55:49.752264Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Without RAG:\n",
      " 广州大学（Guangzhou University）是广东省内一所综合性大学，位于中国广东省广州市。广州大学成立于1952年，前身为广州工学院，是中华人民共和国成立后创建的第一所高等工科院校。\n",
      "广州大学坐落在广州市海珠区，占地面积广阔，校园环境优美。学校拥有多个校区，其中主校区位于广州市番禺区，其他校区分布在广州市的其他地区。学校占地面积约4000亩，拥有现代化的教学、实验和生活设施。\n",
      "广州大学以培养人才为宗旨，注重理论与实践相结合的教学模式。学校开设了多个学院和专业，涵盖了工学、理学、文学、法学、经济学、管理学、艺术学等多个领域。学校现有本科专业近300个，研究生专业涵盖科学、工程、管理、文学、法学、艺术等多个领域。\n",
      "广州大学注重国际交流与合作，积极推进国际化办学。学校与许多国际知名大学建立了合作关系，开展学术交流和合作研究。此外，学校还鼓励学生参与国际交流项目，提供海外实习和留学机会，提升学生的国际视野和能力。\n",
      "广州大学一直以来致力于为学生提供优质的教育环境和丰富的学习资源。学校拥有先进的教学设施和实验室，以及图书馆、体育场馆、艺术工作室等丰富的学生课外活动设施。\n",
      "广州大学以其优秀的教学质量、领先的科研水平和培养优秀学生的能力而闻名。学校致力于培养具有创新精神和社会责任感的高素质人才，为地方经济发展和社会进步做出贡献。<eod>\n",
      "> With RAG:\n",
      " 广州大学是一所位于广东省广州市的全日制普通高等学校，实行省市共建、以市为主的办学体制。学校的办学历史可以追溯到1927年创办的私立广州大学，后来在1951年并入华南联合大学。1984年定名为广州大学。2000年，广州大学经过教育部批准，与广州教育学院、广州师范学院、华南建设学院西院、广州高等师范专科学校合并组建新的广州大学。<eod>\n"
     ]
    }
   ],
   "source": [
    "print('> Without RAG:')\n",
    "llm.generate(question, [])\n",
    "\n",
    "print('> With RAG:')\n",
    "llm.generate(question, context)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
